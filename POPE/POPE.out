--------------------------------------------------------------------------
No OpenFabrics connection schemes reported that they were able to be
used on a specific port.  As such, the openib BTL (OpenFabrics
support) will be disabled for this port.

  Local host:           c3cpu-a2-u30-2
  Local device:         mlx5_0
  Local port:           1
  CPCs attempted:       rdmacm, udcm
--------------------------------------------------------------------------
                       :-) GROMACS - gmx mdrun, 2022 (-:

Executable:   /projects/dora1300/pkgs/gromacs-2022-cpu-mpi/bin/gmx_mpi
Data prefix:  /projects/dora1300/pkgs/gromacs-2022-cpu-mpi
Working dir:  /projects/pafr7911/POPE
Command line:
  gmx_mpi mdrun -deffnm md -cpi md.cpt

Reading file md.tpr, VERSION 2022 (single precision)
Can not increase nstlist because verlet-buffer-tolerance is not set or used
Using 64 MPI processes

Non-default thread affinity set, disabling internal thread affinity

Using 1 OpenMP thread per MPI process

[c3cpu-a2-u30-2:2183116] 63 more processes have sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port
[c3cpu-a2-u30-2:2183116] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
starting mdrun 'Title'
100000000 steps, 200000.0 ps (continuing from step 83944720, 167889.4 ps).

Writing final coordinates.


Dynamic load balancing report:
 DLB was off during the run due to low measured imbalance.
 Average load imbalance: 11.9%.
 The balanceable part of the MD step is 64%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 7.6%.
 Average PME mesh/force load: 1.355
 Part of the total run time spent waiting due to PP/PME imbalance: 11.2 %

NOTE: 7.6 % of the available CPU time was lost due to load imbalance
      in the domain decomposition.
      Dynamic load balancing was automatically disabled, but it might be beneficial to manually tuning it on (option -dlb on.)
      You can also consider manually changing the decomposition (option -dd);
      e.g. by using fewer domains along the box dimension in which there is
      considerable inhomogeneity in the simulated system.
NOTE: 11.2 % performance was lost because the PME ranks
      had more work to do than the PP ranks.
      You might want to increase the number of PME ranks
      or increase the cut-off and the grid spacing.


               Core t (s)   Wall t (s)        (%)
       Time:  1022093.497    15970.215     6400.0
                         4h26:10
                 (ns/day)    (hour/ns)
Performance:      173.720        0.138

GROMACS reminds you: "The only greatness for man is immortality." (James Dean)

